<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	id="analyzers" xmlns="http://www.oasis-open.org/docbook/4.5" xmlns:xl="http://www.w3.org/1999/xlink"
	xsi:schemaLocation="http://www.oasis-open.org/docbook/4.5 http://www.oasis-open.org/docbook/xsd/4.5/docbook.xsd">

	<title>Analyzers</title>

	<chapterinfo>
		<abstract>
			<para>This chapter deals with one of the most important concepts in
				DataCleaner: Analyzers. Analyzers are the endpoints of any analysis
				job, meaning that a job requires at least on analyzer.
			</para>
			<para>An analyzer consumes a (set of) column(s) and generates an
				analysis result based on the values in the consumed columns.
			</para>
			<para>Here is an example of a configuration panel pertaining to an
				analyzer:
			</para>
			<mediaobject>
				<imageobject>
					<imagedata fileref="analysis_job_04_analyzer.jpg" format="JPG"
						scalefit="1" />
				</imageobject>
			</mediaobject>
			<para>In the panel there will always be one or more selections of
				columns. The configuration panel may also contain additional
				properties for configuration.
			</para>
		</abstract>
	</chapterinfo>

	<section id="completeness_analyzer">
		<title>Completeness analyzer</title>
		<para>The completeness analyzer provides a really simple way to check
			that all required fields in your records have been filled. Think of
			it like a big "not null" check across multiple fields. In combination
			with the monitoring application, this analyzer makes it easy to track
			which records needs additional information.
		</para>
		<para>Here is a screenshot of the configuration panel of the
			Completeness analyzer:
		</para>
		<mediaobject>
			<imageobject>
				<imagedata fileref="analyzer_reference_completeness_analyzer.jpg"
					format="JPG" scalefit="1" />
			</imageobject>
		</mediaobject>
		<para>The configuration properties of the Completeness analyzer are:
		</para>
		<table>
			<title>Completeness analyzer properties</title>
			<tgroup cols="2">
				<thead>
					<row>
						<entry>Property</entry>
						<entry>Description</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>Values</entry>
						<entry>Select the columns you want to evaluate with your
							completeness analyzer. For each selected column you get to choose
							whether the analyzer should simply do a null-check, or if it
							should also check for blank values.
						</entry>
					</row>
					<row>
						<entry>Evaluation mode</entry>
						<entry>
							This determines the mode that the completeness check runs in.
							Here you can configure whether the analyzer should consider
							records as "incomplete" if
							<emphasis>any</emphasis>
							of the selected values are null/blank, or if
							<emphasis>all</emphasis>
							the values need to be null/blank before the record is counted as
							incomplete.
						</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
	</section>

	<section id="referential_integrity_analyzer">
		<title>Referential integrity</title>
		<para>With the 'Referential integrity' analyzer you can check that key
			relationships between records are intact. The analyzer will work with
			relationships within a single table, between tables and even between
			tables of different datastores.
		</para>
		<para>Here is a screenshot of the configuration panel of the
			Referential integrity analyzer:
		</para>
		<mediaobject>
			<imageobject>
				<imagedata fileref="analyzer_reference_referential_integrity.jpg"
					format="JPG" scalefit="1" />
			</imageobject>
		</mediaobject>
		<para>
			Apply the analyzer on the table with the foreign key in the
			relationship, and configure it to do a check on the table that holds
			all the valid keys.
		</para>
		<table>
			<title>Referential integrity properties</title>
			<tgroup cols="2">
				<thead>
					<row>
						<entry>Property</entry>
						<entry>Description</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>Cache lookups</entry>
						<entry>Whether or not the analyzer should speed up referential
							integrity checking by caching previous lookup results. Whether or
							not this will gain performance ultimately depends on the amount
							of repetition in the keys to be checked. If all foreign key
							values are more or less unique, it should definately be turned
							off. But if there is a fair amount of duplication in the foreign
							keys (e.g. orderlines referring to the same products or
							customers), then it makes the lookups faster.
						</entry>
					</row>
					<row>
						<entry>Ignore null values</entry>
						<entry>Defines whether or not "null" values should be ignored or
							if they should be considered as an integrity issue. When ignored,
							all records with null foreign key values will simply be discarded
							by the analyzer.
						</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
	</section>

	<section id="duplicate_detection">
		<title>
			Duplicate detection
			<inlinemediaobject>
				<imageobject>
					<imagedata fileref="notice_commercial_editions_only.png"
						format="PNG" />
				</imageobject>
			</inlinemediaobject>
		</title>

		<para>The 'Duplicate detection' function allows you to do fuzzy
			matching of duplicate
			records
			- records that represent the same person,
			organization, product or
			other entity.
		</para>

		<mediaobject>
			<imageobject>
				<imagedata fileref="analyzer_reference_duplicate_detection_menu.jpg"
					format="JPG" />
			</imageobject>
		</mediaobject>

		<para>The main characteristics of the Duplicate detection
			function is:
		</para>

		<orderedlist>
			<listitem>
				<para>
					<emphasis>High Quality</emphasis>
					- Quality is the hallmark of matching, our duplicate detection
					feature delivers on this promise.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>Scalable</emphasis>
					- For large datasets Duplicate detection leverages the Hadoop
					framework for practically unlimited scalability.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>Fast and interactive</emphasis>
					- On a single machine you can work quickly and interactively to
					refine your duplicate detection model.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>International</emphasis>
					- International data is supported and no regional knowledge has
					been encoded into the deduplication engine - you provide the
					business rules externally.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>Machine Learning based</emphasis>
					- The Duplicate detection engine is configured by examples. During
					a series of training sessions you can refine the deduplication
					model simply by having a conversation with the tool about what is
					and what isn't a good example of a duplicate.
				</para>
			</listitem>
		</orderedlist>

		<tip>
			<para>Duplicate detection does work fine with raw data. But if you
				have dirty data and the way data is registered has a lot of
				variance, we suggest you first do your best to standardize the data
				before finding duplicates.
			</para>
			<para>Standardization can be made by trimming, tokenizing, removing
				unwanted characters, replace synonyms and things like that. Explore
				the transformations available in DataCleaner in order to get your
				data cleansed before trying to deduplicate it.
			</para>
		</tip>

		<para>In the following sections we will walk through how to use the
			'Duplicate detection' function. The function has three modes: Model
			training, Detection and Untrained detection.
		</para>

		<section>
			<title>
				'Model training' mode
				<inlinemediaobject>
					<imageobject>
						<imagedata fileref="notice_commercial_editions_only.png"
							format="PNG" />
					</imageobject>
				</inlinemediaobject>
			</title>

			<para>
				In the Model training mode the user of Duplicate detection is
				looking to
				<emphasis>train</emphasis>
				the Machine Learning engine. When running your job in Model training
				mode you will be shown a
				number of potential duplicate record pairs,
				and determine if they
				are duplicate or not.
			</para>

			<para>To start the Training mode, simply add the
				function and select
				the columns that you wish to use for matching.
				Additionally you might
				wish to configure:
			</para>

			<table>
				<title>Model training properties</title>
				<tgroup cols="2">
					<thead>
						<row>
							<entry>Property</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Max records for training</entry>
							<entry>The Training tool will keep a random sample of the dataset
								in memory to provide as training input, and an interactive user
								experience. This number determines how many records will be
								selected for this sample.
							</entry>
						</row>
						<row>
							<entry>Key column</entry>
							<entry>If your dataset has a unique key, we encourage you to
								select it using this property. Configuring the key column has
								the benefit that if you wish to export a training reference
								later, it can be re-applied very easily.
							</entry>
						</row>
						<row>
							<entry>Reference file</entry>
							<entry>If you previously created a reference file in the Training
								tool that you wish to reuse, you can select it here.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>

			<para>In contrast to most other analyzers in DataCleaner which shows
				a result screen after execution, the Training mode opens a new
				dialog when started. The training tool dialog allows users to train
				matching models. The top of the dialog contains a button bar. Below
				the button bar, the training tool shows some tab buttons. By default
				the potential duplicates will be shown. For each potential duplicate
				you can toggle the button on the right side to determine if the pair
				is a duplicate or not:
			</para>

			<mediaobject>
				<imageobject>
					<imagedata fileref="analyzer_reference_training_tool.jpg"
						format="JPG" />
				</imageobject>
			</mediaobject>

			<para>You do not need to classify all samples shown. Recommended
				usage is:
			</para>

			<orderedlist>
				<listitem>
					<para>Classify at least 20-30 duplicate pairs or more (more is
						better)
					</para>
				</listitem>
				<listitem>
					<para>Classify at least 20-30 unique records or more (more is
						better)
					</para>
				</listitem>
			</orderedlist>

			<para>Once you've classified records you can press the 'Train model'
				button in the upper right corner. This will refine the matching
				model and present a new set of interesting potential duplicates. You
				can continue this way and quite quickly have classified the required
				amount of pairs.
			</para>

			<para>
				All duplicate detection models may have irregularities. When
				you ask a computer to do a complex task like matching, it may come
				up with a model that has slight differences from your
				classifications. You can inspect the current model's differences
				from your classifications in the tab 'Discrepancies'.
			</para>

			<para>
				Every time you classify a duplicate, it is added to the
				<emphasis>reference</emphasis>
				of the Training session. You can inspect your complete
				reference in
				the tab 'Duplicates reference'.
			</para>

			<para>If you're looking for particular types of duplicate pairs, you
				may want to go to the 'Search pairs' tab. In this tab you will find
				options to search for records with matching or non-matching values
				for particular fields. This may be a very useful shortcut for
				finding proper duplicate examples.
			</para>

			<para>Finally, the tab 'Training parameters' presents buttons and
				sliders for
				you to influence the Machine Learning approach. It
				presents the
				following options:
			</para>

			<table>
				<title>'Training parameters' options</title>
				<tgroup cols="2">
					<thead>
						<row>
							<entry>Parameter</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Only optimize the preselection when sufficient duplicates
								have been marked
							</entry>
							<entry>This will optimize the matching rules using an initial
								model for pre-selection until sufficient pairs have been marked
								as duplicates and uniques. This is the default value.
							</entry>
						</row>
						<row>
							<entry>Always optimize the preselection</entry>
							<entry>This will always train pre-selection and matching rules.
								Training both is slower, but an improved preselection has a
								better performance during
								duplicate detection. Training the
								potential pair generation too early
								might result in the exclusion
								of some groups of duplicate pairs.
								It is recommended to perform
								training of the full model only after the user has
								classified
								varying examples of duplicates and unique records.
							</entry>
						</row>
						<row>
							<entry>Never optimize the preselection</entry>
							<entry>This never trains the pre-selection, but only the matching
								rules.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>

			<para>
				After updating the matching model, the user can continue in 2
				ways. If
				the user is satisfied with the model (few false positives
				and false
				negatives) then he can save the model and start using it in
				duplicate detection. Otherwise, the user must start a new iteration
				of classifying
				more of the presented samples and refining the model
				again.
			</para>
			<para>
				Longer training set typically allows for a more advanced
				matching model,
				capable of handling more corner cases. The false
				negatives and
				false positives lists give a good impression of the
				current state of the
				matching model. The user should continue
				training until the
				differences
				in these lists are acceptable.
			</para>
			<para>
				When finishing the training, save the reference to file. The
				reference
				is the most important part of the Duplicate detection
				model, since it allows you to reproduce and re-train a model based
				on past classifications.
			</para>

			<para>To validate the training results and obtain the best model,
				training can be repeated on a different sample.
			</para>

			<orderedlist>
				<listitem>
					<para>
						Save the reference
					</para>
				</listitem>
				<listitem>
					<para>Close the training tool.</para>
				</listitem>
				<listitem>
					<para>In the Training tool properties, specify the saved reference.
					</para>
				</listitem>
				<listitem>
					<para>Re-run the Training tool. A new sample will be generated. All
						marked pairs in the saved reference are automatically included in
						the new sample.
					</para>
				</listitem>
				<listitem>
					<para>Press the 'Train model' button in the Training tool. This
						will train a model on the existing reference.
					</para>
				</listitem>
				<listitem>
					<para>You can view the discrepancies (false positives, false
						negatives) of the trained model against the records in the new
						sample.
					</para>
				</listitem>
				<listitem>
					<para>You can review the potential duplicates to determine if a
						category of duplicates is missing.
					</para>
				</listitem>
				<listitem>
					<para>Add more pairs to the reference as needed.</para>
				</listitem>
			</orderedlist>

			<para>Use the 'Export model' button to save the current matching
				model. A dialog appears which allows the user to choose the location
				and file name for the model.
			</para>

		</section>

		<section>
			<title>
				'Detection' mode
				<inlinemediaobject>
					<imageobject>
						<imagedata fileref="notice_commercial_editions_only.png"
							format="PNG" />
					</imageobject>
				</inlinemediaobject>
			</title>
			<para>When the matching model is complete you are ready to find all
				the duplicates in the dataset, usie the Duplicate Detection
				analyzer.
			</para>

			<para>Configure the analyzer with the same input fields as your
				Training tool. Load the matching model from the file you saved at
				the end of your training session. When you run the job, you will see
				a Duplicate detection result with complete groups of duplicates,
				like this:
			</para>

			<mediaobject>
				<imageobject>
					<imagedata fileref="analyzer_reference_duplicate_detection.jpg"
						format="JPG" />
				</imageobject>
			</mediaobject>

			<para>Once you have a duplicate detection result that you wish to
				post-process, e.g. for merging or manual inspection, you can export
				the result by clicking one of the 'Export to staging table' or
				'Export to Excel' buttons in the top of the result screen.
			</para>

			<para>The Duplicate detection analyzer can run stand-alone to
				find
				duplicates in datasets up to a half to 1 million records
				(depending
				on the amount of columns). For larger datasets, the
				Duplicate
				detection component can be used in combination with an
				Hadoop server
				component. This server component is a Enterprise
				edition feature
				only.
			</para>

		</section>

		<section>
			<title>
				'Untrained detection' mode
				<inlinemediaobject>
					<imageobject>
						<imagedata fileref="notice_commercial_editions_only.png"
							format="PNG" />
					</imageobject>
				</inlinemediaobject>
			</title>
			<para>Finally, there's also a 'Untrained detection' mode. This allows
				you to skip 'Model training' and just ask the application to do it's
				best effort without any proper model. This mode is not recommended
				for production use and is considered 'experimental', but may provide
				a great quick impression of some of the duplicates you have in your
				dataset.
			</para>
		</section>

	</section>

	<section id="boolean_analyzer">
		<title>Boolean analyzer</title>
		<para>Boolean analyzer is an analyzer targeted at boolean values. For
			a single boolean column it is quite simple: It will show the
			distribution of true/false (and optionally null) values in a column.
			For several columns it will also show the value combinations and the
			frequencies of the combinations. The combination matrix makes the
			Boolean analyzer a handy analyzer for use with combinations of
			matching transformers and other transformers that yield boolean
			values.
		</para>
		<para>Boolean analyzer has no configuration parameters, except for the
			input columns.
		</para>
	</section>

	<section id="character_set_distribution">
		<title>Character set distribution</title>
		<para>The Character set distribution analyzer inspects and maps text
			characters according to character set affinity, such as Latin,
			Hebrew, Cyrillic, Chinese and more.
		</para>
		<para>Such analysis is convenient for getting insight into the
			international aspects of your data. Are you able to read and
			understand all your data? Will it work in your non-internationalized
			systems?
		</para>
	</section>

	<section id="date_dap_analyzer">
		<title>Date gap analyzer</title>
		<para>The Date gap analyzer is used to identify gaps in recorded time
			series. This analyzer is useful for example if you have employee time
			registration systems which record FROM and TO dates. It will allow
			you to identify if there are unexpected gaps in the data.
		</para>
	</section>

	<section id="date_time_analyzer">
		<title>Date/time analyzer</title>
		<para>The Date/time analyzer provides general purpose profiling
			metrics for temporal column types such as DATE, TIME and TIMESTAMP
			columns.
		</para>
	</section>

	<section id="reference_data_matcher_analyzer">
		<title>Reference data matcher</title>
		<para>The 'Reference data matcher' analyzer provides an easy means to
			match several
			columns against several dictionaries and/or several
			string patterns.
			The result is a matrix of match information for all
			columns and all
			matched resources.
		</para>
	</section>

	<section id="number_analyzer">
		<title>Number analyzer</title>
		<para>The number analyzer provides general purpose profiling
			metrics
			for numerical column types.
		</para>
	</section>

	<section id="pattern_finder">
		<title>Pattern finder</title>
		<para>The pattern finder is one of the more advanced, but also very
			popular analyzers of DataCleaner.
		</para>
		<para>Here is a screenshot of the configuration panel of the Pattern
			finder:
		</para>
		<mediaobject>
			<imageobject>
				<imagedata fileref="analyzer_reference_pattern_finder.jpg"
					format="JPG" scalefit="1" />
			</imageobject>
		</mediaobject>
		<para>From the screenshot we can see that the Pattern finder has these
			configuration properties:
		</para>
		<table>
			<title>Pattern finder properties</title>
			<tgroup cols="2">
				<thead>
					<row>
						<entry>Property</entry>
						<entry>Description</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>Group column</entry>
						<entry>Allows you to define a pattern group column. With a pattern
							group column you can separate the identified patterns into
							separate buckets/groups. Imagine for example that you want to
							check if the phone numbers of your customers are consistent. If
							you have an international customer based, you should then group
							by a country column to make sure that phone patterns identified
							are not matched with phone patterns from different countries.
						</entry>
					</row>
					<row>
						<entry>Discriminate text case</entry>
						<entry>Defines whether or not to discriminate (ie. consider as
							different pattern parts) based on text case. If true
							"DataCleaner" and "datacleaner" will be considered instances of
							different patterns, if false they will be matched within same
							pattern.
						</entry>
					</row>
					<row>
						<entry>Discriminate negative numbers</entry>
						<entry>When parsing numbers, this property defines if negative
							numbers should be discriminated from positive numbers.
						</entry>
					</row>
					<row>
						<entry>Discriminate decimals</entry>
						<entry>When parsing numbers, this property defines if decimal
							numbers should be discriminated from integers.
						</entry>
					</row>
					<row>
						<entry>Enable mixed tokens</entry>
						<entry>
							<para>Defines whether or not to categorize tokens that contain
								both letters and digits as "mixed", or alternatively as two
								separate tokens. Mixed tokens are represented using questionmark
								('?')
								symbols.
							</para>
							<para>This is one of the more important configuration properties.
								For example if mixed tokens are enabled (default), all these
								values will
								be matched against the same pattern: foo123, 123foo,
								foobar123, foo123bar. If mixed tokens are NOT enabled only
								foo123 and foobar123 will be matched (because 123foo and
								foo123bar represent different combinations of letter and digit
								tokens).
							</para>
						</entry>
					</row>
					<row>
						<entry>Ignore repeated spaces</entry>
						<entry>Defines whether or not to discriminate based on amount of
							whitespaces.
						</entry>
					</row>
					<row>
						<entry>Upper case patterns expand in size</entry>
						<entry>Defines whether or not upper case tokens automatically
							"expand" in size. Expandability refers to whether or not the
							found patterns will include matches if a candidate has the same
							type of token, but with a different size. The default
							configuration for upper case characters is false (ie. ABC is
							not
							matched with ABCD).
						</entry>
					</row>
					<row>
						<entry>Lower case patterns expand in size</entry>
						<entry>
							<para>Defines whether or not lower case tokens automatically
								"expand" in size. As with upper case expandability, this
								property
								refers to whether or not the found patterns will include
								matches
								if a candidate has the same type of token, but with a
								different
								size. The default configuration for lower case
								characters is
								true
								(ie. 'abc' is not matched with 'abc').
							</para>
							<para>The defaults in the two "expandability" configuration
								properties mean that eg. name pattern recognition is meaningful:
								'James' and 'John' both pertain to the same pattern ('Aaaaa'),
								while 'McDonald' pertain to a different pattern ('AaAaaaaa').
							</para>
						</entry>
					</row>
					<row>
						<entry>Predefined token name</entry>
						<entry>Predefined tokens make it possible to define a token to
							look for and classify using either just a fixed list of values or
							regular expressions. Typically this is used if the values contain
							some additional parts which you want to manually define a
							matching category for. The 'Predefined token name' property
							defines the name of such a category.
						</entry>
					</row>
					<row>
						<entry>Predefined token regexes</entry>
						<entry>Defines a number of string values and/or regular
							expressions which are used to match values against the
							(pre)defined token category.
						</entry>
					</row>
					<row>
						<entry>Decimal separator</entry>
						<entry>The decimal separator character, used when parsing numbers
						</entry>
					</row>
					<row>
						<entry>Thousand separator</entry>
						<entry>The thousand separator character, used when parsing numbers
						</entry>
					</row>
					<row>
						<entry>Minus sign</entry>
						<entry>The minus sign character, used when parsing numbers</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
	</section>

	<section id="string_analyzer">
		<title>String analyzer</title>
		<para>The string analyzer provides general purpose profiling
			metrics
			for string column types. Of special concern to the string analyzer is
			the amount of words, characters, special signs, diacritics and other
			metrics that are virtal to understanding what kind of string values
			occur in the data.
		</para>
	</section>

	<section id="value_distribution">
		<title>Value distribution</title>
		<para>The value distribution (often also referred to as 'Frequency
			analysis') allows you to identify all the values of a particular
			column. Furthermore you can investigate which rows pertain to
			specific values.
		</para>
		<para>Here are the configuration properties for the value distribution
			analyzer:
		</para>
		<table>
			<title>Value distribution properties</title>
			<tgroup cols="2">
				<thead>
					<row>
						<entry>Property</entry>
						<entry>Description</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>Group column</entry>
						<entry>Allows you to define a column for grouping the result. With
							a
							group column you can separate the identified value distributions
							into
							separate buckets/groups. Imagine for example that you want to
							check if the postal codes and city names correspond or if you
							just want to segment your value distribution on eg. country or
							gender or ...
						</entry>
					</row>
					<row>
						<entry>Record unique values</entry>
						<entry>By default all unique values will be included in the result
							of the value distribution. This can potentially cause memory
							issues if your analyzed columns contains a LOT of unique values
							(eg. if it's a unique key). If the actual unique values are not
							of interest, then uncheck this checkbox to only count (but not
							save for inspection) the unique values.
						</entry>
					</row>
					<row>
						<entry>Top n most frequent vales</entry>
						<entry>An optional number used if the analysis should only display
							eg. the "top 5 most frequent values". The result of the analysis
							will only contain top/bottom n most frequent values, if this
							property is supplied.
						</entry>
					</row>
					<row>
						<entry>Bottom n most frequent values</entry>
						<entry>An optional number used if the analysis should only display
							eg. the "bottom 5 most frequent values". The result of the
							analysis
							will only contain top/bottom n most frequent values, if
							this
							property is supplied.
						</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
	</section>

	<section id="value_matcher">
		<title>Value matcher</title>
		<para>
			The value matcher works very similar to the
			<link linkend="value_distribution">Value distribution</link>
			, except for the fact that it takes a list of expected values and
			everything else is put into a group of 'unexpected values'. This
			division of values means a couple of things:
		</para>
		<orderedlist>
			<listitem>
				<para>You get a built-in validation mechanism. You expect maybe only
					'M' and 'F' values for your 'gender' column, and everything else is
					in a sense invalid, since it is unexpected.
				</para>
			</listitem>
			<listitem>
				<para>The division makes it easier to monitor specific values in the
					data quality monitoring web application.
				</para>
			</listitem>
			<listitem>
				<para>This analyzer scales much better for large datasets, since the
					groupings are deterministic and thus can be prepared for in the
					batch run.
				</para>
			</listitem>
		</orderedlist>
	</section>

	<section id="weekday_distribution">
		<title>Weekday distribution</title>
		<para>The weekday distribution provides a frequency analysis for date
			columns, where you can easily identify which weekdays a date field
			represents.
		</para>
	</section>

</chapter>
