<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	id="improve" xmlns="http://www.oasis-open.org/docbook/4.5" xmlns:xl="http://www.w3.org/1999/xlink"
	xsi:schemaLocation="http://www.oasis-open.org/docbook/4.5 http://www.oasis-open.org/docbook/xsd/4.5/docbook.xsd">

	<title>Improve</title>

	<chapterinfo>
		<abstract>
			<para>The functions in the 'Improve' tree are all what we would
				describe as first class 'Data Quality functions'. They not only
				analyze a problem but also usually present a solution to it.
			</para>
		</abstract>
	</chapterinfo>

	<section id="duplicate_detection">
		<title>
			Duplicate detection
			<inlinemediaobject>
				<imageobject>
					<imagedata fileref="notice_commercial_editions_only.png"
						format="PNG" />
				</imageobject>
			</inlinemediaobject>
		</title>

		<para>The 'Duplicate detection' function allows you to do fuzzy
			matching of duplicate
			records
			- records that represent the same person,
			organization, product or
			other entity.
		</para>

		<mediaobject>
			<imageobject>
				<imagedata fileref="analyzer_reference_duplicate_detection_menu.jpg"
					format="JPG" />
			</imageobject>
		</mediaobject>

		<para>The main characteristics of the Duplicate detection
			function is:
		</para>

		<orderedlist>
			<listitem>
				<para>
					<emphasis>High Quality</emphasis>
					- Quality is the hallmark of matching, our duplicate detection
					feature delivers on this promise.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>Scalable</emphasis>
					- For large datasets Duplicate detection leverages the Hadoop
					framework for practically unlimited scalability.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>Fast and interactive</emphasis>
					- On a single machine you can work quickly and interactively to
					refine your duplicate detection model.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>International</emphasis>
					- International data is supported and no regional knowledge has
					been encoded into the deduplication engine - you provide the
					business rules externally.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>Machine Learning based</emphasis>
					- The Duplicate detection engine is configured by examples. During
					a series of training sessions you can refine the deduplication
					model simply by having a conversation with the tool about what is
					and what isn't a good example of a duplicate.
				</para>
			</listitem>
		</orderedlist>

		<tip>
			<para>Duplicate detection does work fine with raw data. But if you
				have dirty data and the way data is registered has a lot of
				variance, we suggest you first do your best to standardize the data
				before finding duplicates.
			</para>
			<para>Standardization can be made by trimming, tokenizing, removing
				unwanted characters, replace synonyms and things like that. Explore
				the transformations available in DataCleaner in order to get your
				data cleansed before trying to deduplicate it.
			</para>
		</tip>

		<para>In the following sections we will walk through how to use the
			'Duplicate detection' function. The function has three modes: Model
			training, Detection and Untrained detection.
		</para>

		<section>
			<title>
				'Model training' mode
			</title>

			<para>
				In the Model training mode the user of Duplicate detection is
				looking to
				<emphasis>train</emphasis>
				the Machine Learning engine. When running your job in Model training
				mode you will be shown a
				number of potential duplicate record pairs,
				and determine if they
				are duplicate or not.
			</para>

			<para>To start the Training mode, simply add the
				function and select
				the columns that you wish to use for matching.
				Additionally you might
				wish to configure:
			</para>

			<table>
				<title>Model training properties</title>
				<tgroup cols="2">
					<thead>
						<row>
							<entry>Property</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Max records for training</entry>
							<entry>The Training tool will keep a random sample of the dataset
								in memory to provide as training input, and an interactive user
								experience. This number determines how many records will be
								selected for this sample.
							</entry>
						</row>
						<row>
							<entry>Key column</entry>
							<entry>If your dataset has a unique key, we encourage you to
								select it using this property. Configuring the key column has
								the benefit that if you wish to export a training reference
								later, it can be re-applied very easily.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>

			<para>In contrast to most other analyzers in DataCleaner which shows
				a result screen after execution, the Training mode opens a new
				dialog when started. The training tool dialog allows users to train
				matching models. The top of the dialog contains a button bar. Below
				the button bar, the training tool shows some tab buttons. By default
				the potential duplicates will be shown. For each potential duplicate
				you can toggle the button on the right side to determine if the pair
				is a duplicate or not:
			</para>
			
			<mediaobject>
				<imageobject>
					<imagedata fileref="analyzer_reference_training_tool.png"
						format="JPG" />
				</imageobject>
			</mediaobject>

			<para>To help you, columns with equal values are shown in a grey font, while
				different values are shown in black.
			</para>

			<para>Right-clicking on the classification button opens a small menu that allows you to
				mark all examples on the (remainder of) this page or all examples on all pages as
				Undecided, duplicates or uniques. This helps when almost all examples are duplicates
				or uniques. You can mark all examples as duplicates, review, and only toggle the
				examples that are no duplicates.
			</para>
			
			<para>You do not need to classify all samples shown. Recommended
				usage is:
			</para>

			<orderedlist>
				<listitem>
					<para>Classify at least 20-30 duplicate pairs or more (more is
						better)
					</para>
				</listitem>
				<listitem>
					<para>Classify at least 20-30 unique records or more (more is
						better)
					</para>
				</listitem>
			</orderedlist>

			<para>Once you've classified records you can press the 'Train model'
				button in the upper right corner. This will refine the matching
				model and present a new set of interesting potential duplicates. You
				can continue this way and quite quickly have classified the required
				amount of pairs.
			</para>

			<para>The model is automatically saved every time after training. There
				is no need to save the model by hand. The saved model includes the
				matching rules, settings, and all pairs the user classified as duplicate or unique.
			</para>
			
			<para>Some more hints for training:
			</para>
			
			<orderedlist>
				<listitem>
					<para>Classifying uniques is just as important as classifying
					duplicates. Keep the numbers of duplicate examples and unique
					examples roughly equal.
					</para>
				</listitem>
				<listitem>
					<para>Try to find and mark some examples of every duplicate 
					category that you know of. You can use the "search pairs" tool to help you.
					</para>
				</listitem>
				<listitem>
					<para>Sometimes the machine learning gets skewed and does not provide
						examples of a category of duplicate records or unique records.
						In those cases, close then re-open the training tool as described below, but do
						<emphasis>not</emphasis> press the train model button yet. The training
						tool shows a less specialised set of duplicate samples. You should now be
						able to find examples of the category you need added to the model.
					</para>
				</listitem>
			</orderedlist>
			
			<para>
				All duplicate detection models may have irregularities. When
				you ask a computer to do a complex task like matching, it may come
				up with a model that has slight differences from your
				classifications. You can inspect the current model's differences
				from your classifications in the tab 'Discrepancies'.
			</para>

			<para>
				Every time you classify a duplicate, it is added to the
				<emphasis>reference</emphasis>
				of the Training session. You can inspect your complete
				reference in
				the tab 'Duplicates reference'.
			</para>

			<para>If you're looking for particular types of duplicate pair examples, you
				may want to go to the 'Search pairs' tab. In this tab you will find
				options to search for records with matching or non-matching values
				for particular fields. This may be a very useful shortcut for
				finding proper duplicate examples.
			</para>

			<para>Finally, the tab 'Training parameters' presents buttons and
				sliders for you to influence the Machine Learning approach.
			</para>
			
			<para>Moving the top slider to the left makes duplicate detection compare
				more records. This will take more time, but also increase the matching
				quality. Moving this slider to the right makes duplicate detection to
				make less comparisons, resulting in higher speed, but can lead to more
				missed matches (false negatives). 
			</para>

			<para>Moving the bottom slider to the left makes comparison of records more
			    strict. Moving this slider to the right makes it more lax.
			</para>

			<para>The user defined rules enable you to enforce fixed rules. The possible
			     types of fixed rules are listed below. You can apply a rule to each column. 
			     Rules that force a pair to be unique take precedence over rules that force 
			     a pair to be duplicate. Empty values count as different.
			</para>

			<orderedlist>
				<listitem>
					<para>forces pairs to be duplicate when equals - The pair is always 
						a duplicate if any column marked with this value is equal.
					</para>
				</listitem>
				<listitem>
					<para>forces pairs to be unique when equals - The pair is never
						a duplicate if any columns marked with this value is different.
					</para>
				</listitem>
				<listitem>
					<para>forces pairs to be unique when different - The pair is never
						a duplicate if any columns marked with this value is different
					</para>
				</listitem>
				<listitem>
					<para>forces pairs to be duplicate when equals and unique when different -
						The pair is never a duplicate if any columns marked with this value is different, 
						but the pairs is always a duplicate if all columns marked with this value are equal.
					</para>
				</listitem>
				<listitem>
					<para>forces pairs to be unique when equals and unique when different - The pair is
						never a duplicate unless the value in one of the records is empty.
					</para>
				</listitem>
			</orderedlist>
			
			<para>We recommend applying fixed rules only after training the model and only
				when strictly necessary.
			</para>
			
			<para>
				After updating the matching model, the user can continue in 2
				ways. If
				the user is satisfied with the model (few false positives
				and false
				negatives) then he can save the model and start using it in
				duplicate detection. Otherwise, the user can classify
				more of the presented samples and refine the model
				again.
			</para>
			<para>
				More training typically allows for a more advanced
				matching model,
				capable of handling more corner cases. The false
				negatives and
				false positives lists give a good impression of the
				current state of the
				matching model. The user should continue
				training until the
				differences
				in these lists are acceptable.
			</para>

			<para>To validate the training results and obtain the best model,
				training can be repeated on a different sample. The already
				classified record pairs will automatically be added to the
				new sample.
			</para>

			<orderedlist>
				<listitem>
					<para>Close the training tool.</para>
				</listitem>
				<listitem>
					<para>Re-run the Training tool. A new sample will be generated. All
						marked pairs in the saved reference are automatically included in
						the new sample.
					</para>
				</listitem>
				<listitem>
					<para>Press the 'Train model' button in the Training tool. This
						will train a model on the existing reference.
					</para>
				</listitem>
				<listitem>
					<para>You can view the discrepancies (false positives, false
						negatives) of the trained model against the records in the new
						sample.
					</para>
				</listitem>
				<listitem>
					<para>You can review the potential duplicates to determine if a
						category of duplicates is missing.
					</para>
				</listitem>
				<listitem>
					<para>Add more pairs to the reference as needed.</para>
				</listitem>
			</orderedlist>

		</section>

		<section>
			<title>
				'Detection' mode
			</title>
			<para>When the matching model is complete you are ready to find all
				the duplicates in the dataset. Use the same Duplicate detection
				component, but change the execution mode in "Duplicate detection".  
			</para>

			<para>When you run the job, you will see
				a Duplicate detection result with complete groups of duplicates,
				like this:
			</para>

			<mediaobject>
				<imageobject>
					<imagedata fileref="analyzer_reference_duplicate_detection.png"
						format="JPG" />
				</imageobject>
			</mediaobject>

			<para>Once you have a duplicate detection result that you wish to
				post-process, e.g. for merging or manual inspection, you can export
				the result by clicking the 'Write duplicates' button in the top of the result screen.
				You can save the duplicate records, the duplicate pairs and also the unique records in
				a datastore table of your choice. Or you can create an excel file or a staging table.
			</para>

			<para>The Duplicate detection analyzer can run stand-alone to
				find
				duplicates in datasets up to a half to 1 million records
				(depending
				on the amount of columns). For larger datasets, the
				Duplicate
				detection component can be used in combination with an
				Hadoop server
				component. This server component is an Enterprise
				edition feature.
			</para>

		</section>

		<section>
			<title>
				'Untrained detection' mode
			</title>
			<para>Finally, there's also a 'Untrained detection' mode. This allows
				you to skip 'Model training' and just ask the application to do it's
				best effort without any proper model. This mode is not recommended
				for production use and is considered 'experimental', but may provide
				a great quick impression of some of the duplicates you have in your
				dataset.
			</para>
		</section>

	</section>

	<section id="merge_duplicates">

		<title>
			Merge duplicates
			<inlinemediaobject>
				<imageobject>
					<imagedata fileref="notice_commercial_editions_only.png"
						format="PNG" />
				</imageobject>
			</inlinemediaobject>
		</title>

		<para>Merging duplicates is the next step after detecting them in a
			dataset. It helps to restore a single version of truth by combining
			information from all the duplicate records representing the same
			physical entity.
		</para>

		<para>
			In this section, we assume that the steps from previous section
			"Duplicate detection" are completed and the duplicates are available
			in a staging datastore "Duplicates export".
		</para>

		<section>
			<title>
				Copy the unique records
			</title>

			<para>
				Before we get into merging the duplicates, let's make sure all
				the unique entries in our datasets are propagated to the new
				(cleansed) dataset.
			</para>

			<para>
				Open the original dataset.
				Apply "Table lookup" transformer to
				it and configure "Table lookup" to
				check if a record is present in
				the
				results of duplicate detection
				staging table (lookup in RECORDS
				table). Choose a unique identifier
				column as condition value (in
				this
				case: CUSTOMERNUMBER). As an
				output column choose GROUPID, which is
				an additional
				information in
				RECORDS table.
			</para>

			<mediaobject>
				<imageobject>
					<imagedata fileref="merge_duplicates_table_lookup_properties.png"
						format="PNG" />
				</imageobject>
			</mediaobject>

			<para>
				Table lookup will output the value of GROUPID or NULL. We are
				interested in NULL values as it means the record has not been
				found
				in duplicates table, so this is the unique record we would
				like to
				transfer to the final dataset. Therefore, we add Null check
				filter to
				save only the unique records (GROUPID is equal to NULL).
			</para>

			<mediaobject>
				<imageobject>
					<imagedata fileref="merge_duplicates_null_check.png"
						format="PNG" />
				</imageobject>
			</mediaobject>

			<para>
				Save the output of the filter to a a new staging table.
				<mediaobject>
					<imageobject>
						<imagedata fileref="merge_duplicates_create_staging_table_properties.png"
							format="PNG" />
					</imageobject>
				</mediaobject>
			</para>

			<para>
				The whole job should look
				like this:
			</para>

			<mediaobject>
				<imageobject>
					<imagedata fileref="merge_duplicates_unique_job.png"
						format="PNG" />
				</imageobject>
			</mediaobject>

		</section>

		<section>
			<title>
				Merge duplicates
			</title>

			<para>Now it is time to merge the duplicates into one record in our
				final dataset.
			</para>

			<para>In the Improve menu, in the
				Deduplication submenu, there are two
				components that are useful for
				this task - Merge duplicates (simple)
				and Merge duplicates (advanced).
				The simple version just picks one
				record from the group of duplicates
				while the advanced one enables
				the
				user to combine records, taking
				some values from one record, some
				from another. In this example we
				will use the simple version.
			</para>

			<para>
				Open the duplicates staging datastore as the job source
				datastore.
				Apply
				"Merge duplicates (Simple)" transformer to the
				RECORDS table
				from the duplicates datastore. Select all the relevant
				columns as
				input (in our case all of them) and select GROUPID column
				in "Group
				id" dropdown menu and GROUPSIZE column in "Group count".
			</para>

			<mediaobject>
				<imageobject>
					<imagedata fileref="merge_duplicates_merge_simple_properties.png"
						format="PNG" />
				</imageobject>
			</mediaobject>

			<para>
				Merge duplicates (Simple) transformer will output all the input
				columns as output + additional colums carrying metadata about the
				record. One of them is "Merge status" that can have two possible
				values: SURVIVOR and NON_SURVIVOR. SURVIVOR indicates the record
				that has been chosen as the final one. Let's add an Equals filter
				to
				the job in order to
				write only these SURVIVOR records into our
				final
				dataset.
			</para>

			<mediaobject>
				<imageobject>
					<imagedata fileref="merge_duplicates_equals_filter_properties.png"
						format="PNG" />
				</imageobject>
			</mediaobject>

			<para>
				Save the output of the filter to a the staging table created in
				the
				previous section (where the unique records had been written).
			</para>
			<para>
				The whole job graph should look similar to this:
			</para>

			<mediaobject>
				<imageobject>
					<imagedata fileref="merge_duplicates_merge_job.png"
						format="PNG" />
				</imageobject>
			</mediaobject>

		</section>

		<section>
			<title>
				Conclusion
			</title>

			Following above sections, we obtained a datastore with unique values
			and merged duplicate inside, ready to be exported to the format of
			user's
			preference.

		</section>

	</section>

	<section id="synonym_lookup_transformer">
		<title>Synonym lookup</title>
		<para>
			The Synonym lookup transformation is a critical part of DataCleaner's
			ability to standardize and cleanse data. Using this component you can
			look up values in a
			<link linkend="synonym_catalogs">synonym catalog</link>
			and have it replaced with it's master term, if it is found to be a
			synonym.
		</para>
		<para>Below is a screenshot of the synonym lookup's configuration
			panel:
		</para>
		<mediaobject>
			<imageobject>
				<imagedata fileref="transformer_reference_synonym_lookup.png"
					format="PNG" scalefit="1" />
			</imageobject>
		</mediaobject>
		<para>
			The configuration of the Synonym lookup is simple:
		</para>
		<orderedlist>
			<listitem>
				<para>Select the column to apply the lookup function to.</para>
			</listitem>
			<listitem>
				<para>Use the 'Retain original value' option to determine if
					unmatched values (non-synonyms) should be retained or if a null
					value should be returned if there is no match.
				</para>
			</listitem>
			<listitem>
				<para>Select the synonym catalog to use for lookups.</para>
			</listitem>
		</orderedlist>

		<para>
			If your synonym catalog contains all the allowed values for a
			particula column, it can be a good idea to uncheck the 'Retain
			original value' checkbox and then do a simple null-check on the
			resulting output column. If null values are found, it's because there
			are values in the column that the synonym catalog is not able to
			standardize.
		</para>
	</section>

	<section id="DE_suppression">
		<title>
			DE Address Correction/Suppression
			<inlinemediaobject>
				<imageobject>
					<imagedata fileref="notice_commercial_editions_only.png"
						format="PNG" />
				</imageobject>
			</inlinemediaobject>
		</title>
		<para>This component provides Address Correction and Suppression
			services for Germany. Use it to check that the name and address data
			you have about people is up to date and correct. The following
			Address suppression checks currently exist:
		</para>
		<orderedlist>
			<listitem>
				<para>Change of Address (with new address) check</para>
			</listitem>
			<listitem>
				<para>Moved Away (without new address) check</para>
			</listitem>
			<listitem>
				<para>Deceased check</para>
			</listitem>
		</orderedlist>
		<tip>
			<para>Customers need a set of credentials to access the service.
			</para>
		</tip>
		<section>
			<title>
				Address and suppression data sources
			</title>
			<para>The service combines data of several sources, including
				Deutsche
				Post.
			</para>
		</section>
		<section>
			<title>
				Output
			</title>
			<para>This component outputs name and address fields, in addition to
				the following indicators:
			</para>

			<table>
				<title>DE Address Correction/Suppression output</title>
				<tgroup cols="2">
					<thead>
						<row>
							<entry>Output column</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Address validation status</entry>
							<entry>
								The status code of the record. Following values are possible:
								<orderedlist>
									<listitem>
										Valid
									</listitem>
									<listitem>
										Corrected to valid
									</listitem>
									<listitem>
										Ambigiuous
									</listitem>
									<listitem>
										Invalid
									</listitem>
									<listitem>
										Not processed
									</listitem>
									<listitem>
										Failure
									</listitem>
									<listitem>
										Unknown
									</listitem>
								</orderedlist>
							</entry>
						</row>
						<row>
							<entry>Is moved?</entry>
							<entry>A boolean flag indicating if the person has moved to a
								new
								address.
							</entry>
						</row>
						<row>
							<entry>Is moved to unknown address?</entry>
							<entry>A boolean flag indicating if the person has moved away
								(with no known new address).
							</entry>
						</row>
						<row>
							<entry>Is deceased?</entry>
							<entry>A boolean flag indicating if the person is deceased.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
		</section>
	</section>

	<section id="UK_suppression">
		<title>
			UK Address Correction/Suppression
			<inlinemediaobject>
				<imageobject>
					<imagedata fileref="notice_commercial_editions_only.png"
						format="PNG" />
				</imageobject>
			</inlinemediaobject>
		</title>
		<para>This component provides Address Correction and Suppression
			services for the United Kingdom / Great Britain. Use it to check that
			the name and address data you have about people is up to date and
			correct. The following Address suppression checks currently exist:
		</para>
		<orderedlist>
			<listitem>
				<para>Change of Address (with new address) check</para>
			</listitem>
			<listitem>
				<para>Moved Away (without new address) check</para>
			</listitem>
			<listitem>
				<para>Deceased check</para>
			</listitem>
			<listitem>
				<para>Mailing Preference Service (MPS) check</para>
			</listitem>
		</orderedlist>
		<tip>
			<para>Customers need a set of credentials to access the service.
			</para>
		</tip>
		<section>
			<title>
				Address and suppression data sources
			</title>
			<para>The service combines data of several sources, including
				Royal
				Mail, MPS and the Bereavement registry.
			</para>
			<para>The service allows you to get a Price Quotation including
				summary statistics about the expected results before accepting it:
			</para>
			<mediaobject>
				<imageobject>
					<imagedata fileref="analyzer_reference_uksuppression_report.jpg"
						format="JPG" scalefit="1" />
				</imageobject>
			</mediaobject>

		</section>
		<section>
			<title>
				Output
			</title>
			<para>This component outputs name and address fields, in addition to
				the following indicators:
			</para>

			<table>
				<title>DE Address Correction/Suppression output</title>
				<tgroup cols="2">
					<thead>
						<row>
							<entry>Output column</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Address validation status</entry>
							<entry>
								A numeric status code of the record. Following value ranges are
								possible:
								<orderedlist>
									<listitem>
										<para>
											<emphasis>0 - 10</emphasis>
											: The address is valid.
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>11 - 100</emphasis>
											: The address was corrected.
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>101 - 500</emphasis>
											: The address is invalid.
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>700</emphasis>
											: The address was not processed / skipped.
										</para>
									</listitem>
								</orderedlist>
							</entry>
						</row>
						<row>
							<entry>AddressCorrectionMessage</entry>
							<entry>A humanly readable message about the address correction
								outcome.
							</entry>
						</row>
						<row>
							<entry>IsNCOATracked</entry>
							<entry>A boolean flag indicating if the person has moved to a
								new
								address.
							</entry>
						</row>
						<row>
							<entry>IsNCOAFlagged</entry>
							<entry>A boolean flag indicating if the person has moved away
								(with no known new address).
							</entry>
						</row>
						<row>
							<entry>IsDeceased</entry>
							<entry>A boolean flag indicating if the person is deceased.
							</entry>
						</row>
						<row>
							<entry>IsDoNotMail</entry>
							<entry>A boolean flag indicating if the person does not want to
								receive unsolicited mail.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
		</section>
	</section>

	<section id="US_suppression">
		<title>
			US Address Correction/Suppression
			<inlinemediaobject>
				<imageobject>
					<imagedata fileref="notice_commercial_editions_only.png"
						format="PNG" />
				</imageobject>
			</inlinemediaobject>
		</title>
		<para>This component provides CASS Certified(tm) Address Correction
			and Suppression
			services for the United States of America. Use it to
			check that
			the name and address data you have about people is up to
			date and
			correct. The following Address suppression checks currently
			exist:
		</para>
		<orderedlist>
			<listitem>
				<para>Change of Address check</para>
			</listitem>
			<listitem>
				<para>Deceased check</para>
			</listitem>
			<listitem>
				<para>Do-Not-Mail check</para>
			</listitem>
		</orderedlist>
		<tip>
			<para>Customers need a set of credentials to access the service.
			</para>
		</tip>
		<section>
			<title>
				Address and suppression data sources
			</title>
			<para>The service combines data of several sources, including
				the US
				Postal Service.
			</para>
		</section>
		<section>
			<title>
				Output
			</title>
			<para>This component outputs name and address fields, in addition to
				the following indicators:
			</para>

			<table>
				<title>DE Address Correction/Suppression output</title>
				<tgroup cols="2">
					<thead>
						<row>
							<entry>Output column</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Address validation status</entry>
							<entry>
								A numeric status code of the record. Following value ranges are
								possible:
								<orderedlist>
									<listitem>
										<para>
											<emphasis>0 - 10</emphasis>
											: The address is valid.
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>11 - 100</emphasis>
											: The address was corrected.
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>101 - 500</emphasis>
											: The address is invalid.
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>700</emphasis>
											: The address was not processed / skipped.
										</para>
									</listitem>
								</orderedlist>
							</entry>
						</row>
						<row>
							<entry>AddressCorrectionMessage</entry>
							<entry>A humanly readable message about the address correction
								outcome.
							</entry>
						</row>
						<row>
							<entry>EcoaFootnote</entry>
							<entry>
								An indicator value telling what the outcome of the 'Change of
								Address' check was. The following tokens can occur:
								<orderedlist>
									<listitem>
										<para>
											<emphasis>N</emphasis>
											- No change
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>M</emphasis>
											- The party has a new address
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>K</emphasis>
											- The party has moved away without a new address
										</para>
									</listitem>
								</orderedlist>
								<para>Furthermore the field may have a token representing the
									type of party that was identified:
								</para>
								<orderedlist>
									<listitem>
										<para>
											<emphasis>I</emphasis>
											- Individual
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>F</emphasis>
											- Family
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>B</emphasis>
											- Business
										</para>
									</listitem>
								</orderedlist>
							</entry>
						</row>
						<row>
							<entry>IsDeceased</entry>
							<entry>A boolean flag indicating if the person is deceased.
							</entry>
						</row>
						<row>
							<entry>IsDoNotMail</entry>
							<entry>A boolean flag indicating if the person does not want to
								receive unsolicited mail.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
		</section>
	</section>

	<section id="easydq_transformers">
		<title>Name and Address correction</title>
		<para>
			These functions are provided as clients to
			<link xl:href="http://www.easydq.com">EasyDQ.com</link>
			. EasyDQ is an on-demand service for data quality functions.
			DataCleaner provides access to these EasyDQ services,
			but a separate
			account is needed in order
			to take them to any use.
		</para>
		<para>
			Please refer to the
			<link xl:href="http://help.easydq.com/datacleaner">EasyDQ for DataCleaner documentation</link>
			for detailed information about the services provided through
			EasyDQ.
		</para>

		<mediaobject>
			<imageobject>
				<imagedata fileref="transformer_reference_table_easydq_documentation.jpg"
					format="JPG" scalefit="1" />
			</imageobject>
		</mediaobject>

	</section>
</chapter>
